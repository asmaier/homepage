<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>philosophie | Das Blog</title>
    <link>/tag/philosophie/</link>
      <atom:link href="/tag/philosophie/index.xml" rel="self" type="application/rss+xml" />
    <description>philosophie</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sat, 08 Feb 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>philosophie</title>
      <link>/tag/philosophie/</link>
    </image>
    
    <item>
      <title>Auf die Gefahr hin euch ...</title>
      <link>/post/2020-02-08-auf-die-gefahr-hin-euch/</link>
      <pubDate>Sat, 08 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-02-08-auf-die-gefahr-hin-euch/</guid>
      <description>&lt;p&gt;Auf die Gefahr hin euch alle zu überfordern hier ein Interview mit einem der tiefsten Denker unserer Zeit: Jürgen Schmidhuber.&lt;/p&gt;
&lt;p&gt;In diesem Interview erklärt er so nebenbei&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;was Bewusstsein ist und warum auch &amp;ldquo;intelligente Maschinen&amp;rdquo; eines entwickeln müssen&lt;/li&gt;
&lt;li&gt;was wir in Zukunft von superintelligenten Wesen erwarten können und wie wir mit ihnen zusammenleben werden&lt;/li&gt;
&lt;li&gt;Wie die Zukunft unseres Universums aussieht.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Und ich glaube es ist nicht zuviel versprochen wenn ich sage: Alle seine Antworten werden euch überraschen. Es lohnt sich ihm zuzuhören und seinen Gedanken zu folgen.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=3FIo6evmweo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://i.ytimg.com/vi/3FIo6evmweo/maxresdefault.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;
youtube.com&lt;/p&gt;
&lt;h2 id=&#34;juergen-schmidhuber-godel-machines-meta-learning-and-lstms--lex-fridman-podcast-11httpswwwyoutubecomwatchv3fio6evmweo&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=3FIo6evmweo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juergen Schmidhuber: Godel Machines, Meta-Learning, and LSTMs | Lex Fridman Podcast #11&lt;/a&gt;&lt;/h2&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Darüber sollte man mal nachdenken. ...</title>
      <link>/post/2018-04-26-daruber-sollte-man-mal-nachdenken/</link>
      <pubDate>Thu, 26 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-04-26-daruber-sollte-man-mal-nachdenken/</guid>
      <description>&lt;p&gt;Darüber sollte man mal nachdenken.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ted.com&lt;/p&gt;
&lt;h2 id=&#34;update-browserhttpswwwtedcomtalksjason_fried_why_work_doesn_t_happen_at_work&#34;&gt;&lt;a href=&#34;https://www.ted.com/talks/jason_fried_why_work_doesn_t_happen_at_work&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Update Browser&lt;/a&gt;&lt;/h2&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Guter Rat will Weile haben. ...</title>
      <link>/post/2017-03-22-guter-rat-will-weile-haben/</link>
      <pubDate>Wed, 22 Mar 2017 00:00:00 +0000</pubDate>
      <guid>/post/2017-03-22-guter-rat-will-weile-haben/</guid>
      <description>&lt;p&gt;Guter Rat will Weile haben.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;sprichwortrekombinator.de&lt;/p&gt;
&lt;h2 id=&#34;sprichwortrekombinatordehttpsprichwortrekombinatorde&#34;&gt;&lt;a href=&#34;http://sprichwortrekombinator.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sprichwortrekombinator.de&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Nie versiegende Quelle der Weisheit.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Choose wisely and read this ...</title>
      <link>/post/2016-11-22-choose-wisely-and-read-this/</link>
      <pubDate>Tue, 22 Nov 2016 00:00:00 +0000</pubDate>
      <guid>/post/2016-11-22-choose-wisely-and-read-this/</guid>
      <description>&lt;p&gt;Choose wisely and read this article!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://aeon.co/essays/if-you-can-t-choose-wisely-choose-randomly&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://images.aeonmedia.co/images/e486b28f-f0d1-4131-9311-891bfb7461d5/JUNGLE-PATH-FINAL-.jpg?width=1200&amp;amp;quality=75&amp;amp;format=auto&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;
aeon.co&lt;/p&gt;
&lt;h2 id=&#34;if-you-cant-choose-wisely-choose-randomly--aeon-essayshttpsaeoncoessaysif-you-can-t-choose-wisely-choose-randomly&#34;&gt;&lt;a href=&#34;https://aeon.co/essays/if-you-can-t-choose-wisely-choose-randomly&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;If you can’t choose wisely, choose randomly | Aeon Essays&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When your reasons are worse than useless, sometimes the most rational choice is a random stab in the dark&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Superintelligence - A critique of Nick Bostroms arguments</title>
      <link>/post/2015-06-07-superintelligence-critique-of-nick/</link>
      <pubDate>Sun, 07 Jun 2015 23:03:00 +0000</pubDate>
      <guid>/post/2015-06-07-superintelligence-critique-of-nick/</guid>
      <description>
&lt;div dir=&#34;ltr&#34; style=&#34;text-align: left;&#34; trbidi=&#34;on&#34;&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: justify;&#34;&gt;&lt;span style=&#34;text-align: left;&#34;&gt;Why is everyone afraid of artificial intelligence? I&#39;m more afraid of natural human stupidity (which is already infinite according to Einstein!). &amp;nbsp;Superintelligence would just balance that out ;-)&lt;/span&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: justify;&#34;&gt;&lt;span style=&#34;text-align: left;&#34;&gt;But seriously I think Nick Bostrom has some flawed arguments in his talk:&lt;/span&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;&lt;iframe allowfullscreen=&#34;&#34; class=&#34;YOUTUBE-iframe-video&#34; data-thumbnail-src=&#34;https://i.ytimg.com/vi/MnT1xgZgkpk/0.jpg&#34; frameborder=&#34;0&#34; height=&#34;532&#34; src=&#34;https://www.youtube.com/embed/MnT1xgZgkpk?feature=player_embedded&#34; width=&#34;640&#34;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;a name=&#39;more&#39;&gt;&lt;/a&gt;&lt;div style=&#34;text-align: left;&#34;&gt;1. He assumes that the work which is necessary to create a higher and higher intelligent being is linear (or even sublinear) with the IQ. But maybe this is not the case. It could be a lot harder than that. What if it were exponentially more difficult to increase the IQ of an artificial intelligence? Then even if we would manage to have artificial intelligence half as smart as a human in e.g. 30 years, then to get it as smart as humans would take us 60 years. To make a machine twice as smart as humans might well take 120 years, and so on. So having a superintelligence smarter than all humans (if we add their IQs together) together would take 30 * 10 billion years. The universe will end before that is going to happen.&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;2. He assumes a superintelligence would (despite of its superintelligence) stick to its stupid first optimization task (e.g. make all humans smile). So for him a superintelligence is just a very powerful optimization process, which can do everything to achieve its goal. However I believe that a superintelligence would understand that its optimization task is in fact stupid and instead do something more useful. Maybe it would simply refuse to work with stupid humans, would self-destruct, because it understands that the universe is finite and life is without sense. From human experience we know that genius and madness is often close to each other. So stabilizing (preventing it from becoming crazy and selfdestruct) a superintelligent being might be a very difficult task.&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;3. He also assumes that we could not put a superintelligence into a save box. But that would only be possible, if the superintelligence would be capable of violating our known physical laws of e.g. electromagnetism and so on. And that would mean, that a superintelligence is automatically super powerful. This seems unlikely. Even though we humans are smarter than animals, we are not super powerful. Physical laws are also valid for us: If we make a mistake, then some animal might simply eat us, despite their lower intelligence. Superhigh intelligence doesn&#39;t automatically mean invulnerability and freedom of error.&amp;nbsp;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;But lets assume, he is right:&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;- A superintelligence can be created in a reasonable amount of time (&amp;lt; 1000 years)&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;- A superintelligence can be stable and not imediately self-destruct.&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;- A superintelligence can violate our known physical laws, become super powerful and free of error.&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;So why did this then not already happen somewhere in the universe? From his arguments I believe such a superintelligence should already exist somewhere in the universe. But how would we call such a superintelligence? We simply would call it God!&amp;nbsp;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;So all his reasoning seems to boil down to the fact that he assumes God (or Gods) exists in the universe. Some religions might agree with him and can surely&amp;nbsp;give him good advice on how to deal with that situation. &amp;nbsp;&amp;nbsp;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;!--1000--&gt;&lt;/div&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Wissenschaft und Meditation. Das kling ...</title>
      <link>/post/2015-03-01-wissenschaft-und-meditation-das-kling/</link>
      <pubDate>Sun, 01 Mar 2015 00:00:00 +0000</pubDate>
      <guid>/post/2015-03-01-wissenschaft-und-meditation-das-kling/</guid>
      <description>&lt;p&gt;Wissenschaft und Meditation. Das kling interessant.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
